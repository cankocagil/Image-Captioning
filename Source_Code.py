# -*- coding: utf-8 -*-
"""Final_project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AhHBTYkqcNnzBOfoS4jwM_tLvBZ41u7m
"""

from __future__ import print_function, division

# Importing basics:
import numpy as np
import h5py
import matplotlib.pyplot as plt
import pandas as pd

# For image pre/processing:
from PIL import Image
#from skimage.io import imread
from skimage.transform import resize
from skimage import io, transform

# Retrieve and manipulate paths:
import urllib
import requests
import os
import shutil
import pickle


# Performance metrics and train test split:
from sklearn.model_selection import train_test_split

# Useful
from tqdm import tqdm
import time
import copy

# PyTorch's:
import torch
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, utils, datasets, models
import torch.nn as nn
import torch.optim as optim
from torch.optim import lr_scheduler
import torchvision
import torch.nn.functional as F
from torch.autograd import Variable

# To access Google Drive:
from google.colab import drive


# interactive mode
plt.ion()   

# For translation model evaluation

from nltk.translate.bleu_score import sentence_bleu
#from nltk.translate.meteor_score import meteor_score


# Ignore warnings
import warnings
warnings.filterwarnings("ignore")

# PyTorch's versions:
print("PyTorch Version: ",torch.__version__)
print("Torchvision Version: ",torchvision.__version__)
print("NumPy Version: ",np.__version__)
drive.mount("/content/gdrive")

colap_path_train = '/content/gdrive/My Drive/Data/eee443_project_dataset_train.h5'
colap_path_test = '/content/gdrive/My Drive/Data/eee443_project_dataset_test.h5'

# Getting the data:
with h5py.File(colap_path_train,'r') as f:
# Names variable contains the names of training and testing file 
    names = list(f.keys())
    train_cap = f[names[0]][()]
    train_imid = np.array(f[names[1]][()])
    train_imid -=1
    train_url = np.array(f[names[3]][()])

data_dir = '/content/gdrive/My Drive/Data/train'
os.chdir(path = data_dir)

print(train_cap.shape)
print(train_imid.shape)
print(train_imid.max()),
print(train_imid.min())
print(train_url.shape)

X = train_imid
y = train_cap

train_inds, val_inds, train_caps, val_caps = train_test_split(
     X, y, test_size = 0.15, random_state = 42)

train_cap_Str = {}
for i in range(len(train_caps)):
  train_cap_Str[i] = [element for element in train_caps[i] if element != 0]


val_cap_Str = {}
for i in range(len(val_caps)):
  val_cap_Str[i] = [element for element in val_caps[i] if element != 0]

sentence_lens_train = [len(ele) for ele in train_cap_Str.values()]
sentence_lens_val = [len(ele) for ele in val_cap_Str.values()]

print(train_inds.shape)
print(train_caps.shape)
print(val_inds.shape)
print(val_caps.shape)
print(np.isclose(0.15, val_inds.shape[0] /(train_inds.shape[0] + val_inds.shape[0]),5))

class ImageCaptionData(Dataset):
  """ Image Caption dataset  
    Args:
      image_urls (np.ndarray)      : Image URL's.
      img_inds (np.ndarray)        : Indices of the images
      captions (np.ndarray)        : Captions indices for images
      sentence_lens (np.ndarray)   : Each captions length without pads
      transform (callable,optinal) : Transformation to be applied on a sample images

    """

  def __init__(self,image_urls,img_inds,captions,sentence_lens,transform = None):
   

    #self.data_dir = data_dir
    #self.image_list = os.listdir(self.data_dir)
    self.image_urls = image_urls
    self.img_inds = img_inds
    self.captions = captions    
    self.transform = transform
    self.sentence_lens = sentence_lens       


  def __len__(self):
    return self.img_inds.shape[0]

  def __getitem__(self,index):
    """ Get input,label in dict format """
    
    # Check the indices is in the correct format:
    if torch.is_tensor(index):
      index = index.tolist()  
  
    connected = False

    while not connected:

      try:
        # Since we may use the same img more than 1 time, we slice the needed index:
        img_index = self.img_inds[index]

        img_url = self.image_urls[img_index].decode('UTF-8')
        img_name = img_url.split("/")[-1].strip()

        # Feeding needed index to get the path of the image in Google Colab:
        #img_path = self.image_list[img_index]
        #urllib.request.urlretrieve(img_url,img_name)
        # Reading images in PIL format since PyTorch works with PIL or Tensor types:
        img = Image.open(img_name).convert('RGB')
            
        # Reading the corresponding caption:
        caption = self.captions[index]

        sen_len = self.sentence_lens[index]

        connected = True

      except:
        index = np.random.randint(self.img_inds.shape[0]) 
        
      
    # If not works:
    #img = io.imread(img_path)

    # Converting array and normalizing:
    #img = np.array(img) / 255  

    # Image + Caption:
    sample = {'image' : img, 'caption' : caption}


    # Apply transformation to the images:
    # Resize + Normalize + Convert to tensor format
    if self.transform is not None:
      sample = self.transform(sample)

    return sample,sen_len

class Resize(object):
  """ Rescale the images to the expected format. 
      It depends on the CNN model's expected input size

      Args:

        out_size (tuple or int) : It is the desired output size. If the output size
        is given as a tuple format, we matched to dimensions, e.g. if we give (484x676)
        this class returns the image with dimensions with (484x676). After that, if we give
        int value to the Resize class, we output the square of the given integer, e.g. let 
        outsize = 224, this class returns 224x224 resized format.
  """

  def __init__(self,out_size):
    assert isinstance(out_size, (int,tuple))
    self.out_size = out_size

  def __call__(self,sample):
    img, caption =  sample['image'],sample['caption']
    
    # Get height and width dimensions of the img
    #H,W = img.shape[:2]


    if isinstance(self.out_size, int):
      new_H,new_W = self.out_size, self.out_size

    else:
      new_H,new_W = self.out_size
    
    #new_H,new_W = int(new_H),int(new_W)
    
    #resized_img = transform.resize(img,(new_H,new_W))
    #tfsm_resize = transforms.Resize((new_H,new_W),interpolation = Image.NEAREST)
    #resized_img = tfsm_resize(Image.fromarray(img.astype('uint8'), 'RGB'))
    
    tfsm = transforms.Resize((new_H,new_W),interpolation=Image.NEAREST)
                      

    return {'image' : tfsm(img), 'caption' : caption}

class RandomCrop(object):
    """Crop randomly the image in a sample.

    Args:
        output_size (tuple or int): Desired output size. If int, square crop
            is made.
    """

    def __init__(self, output_size):
        assert isinstance(output_size, (int, tuple))
        if isinstance(output_size, int):
            self.output_size = (output_size, output_size)
        else:
            assert len(output_size) == 2
            self.output_size = output_size

    def __call__(self, sample):
        image, caption = sample['image'], sample['caption']

        #h, w = image.shape[:2]
        #new_h, new_w = self.output_size

        #top = np.random.randint(0, h - new_h)
        #left = np.random.randint(0, w - new_w)

        #image = image[top: top + new_h,
                      #left: left + new_w]  

        tsfm = transforms.RandomCrop(self.output_size)  

        return {'image': tsfm(image), 'caption': caption}


class RandomHorizontalFlip(object):
  def __call__(self,sample):
    image, caption = sample['image'], sample['caption']
    in_tsfm = transforms.RandomHorizontalFlip(p=0.5)
    return {'image' : in_tsfm(image),'caption' : caption}

class CenterCrop(object):
  def __init__(self,out_size):
    self.out_size = out_size
    
  def __call__(self,sample):
    image, caption = sample['image'], sample['caption']
    in_tsfm = transforms.CenterCrop(self.out_size)
    return {'image' : in_tsfm(image),'caption' : caption}

class RandomVerticalFlip(object):
  def __call__(self,sample):
    image, caption = sample['image'], sample['caption']
    in_tsfm = transforms.RandomVerticalFlip(p=0.5)
    return {'image' : in_tsfm(image),'caption' : caption}
    #{'image' : in_tsfm(Image.fromarray(np.uint8(image))).convert('RGB'),'caption' : caption}

class RandomResizedCrop(object):
  def __init__(self,out_size):
    self.out_size = out_size

  def __call__(self,sample):
    image, caption = sample['image'], sample['caption']
    in_tsfm = transforms.RandomResizedCrop(self.out_size)
    return {'image' : in_tsfm(image),'caption' : caption}

class ToTensor(object):
  """ From numpy ndarray to PyTorch tensor format 

      Args:
        sample (tuple) : samples contains both image and caption, this class
        apply transformation on images to normalize the ImageNet format since
        we utilizes pre-trained state-of-the-art models for transfer learning 
        and all models are trained in Imagenet dataset. Then, convert both images
        and captions to PyTorch's tensor format.
  """

  def __call__(self,sample):
    img, caption =  sample['image'], sample['caption']

    # PyTorch's expected image size C x H x W
    #formatted_img = np.transpose(img,(2, 0, 1))
    #img = img.tranpose(2, 0, 1)
    #img = np.swapaxes(img,-1,0)
    #img = np.swapaxes(img,1,2)
    tf = transforms.ToTensor()

    #tensor_img = tf(img).float()
    # Convert from numpy ndarray to tensor format
    #tensor_img, tensor_caption = torch.from_numpy(formatted_img).float(), torch.from_numpy(caption)

    tensor_caption = torch.from_numpy(caption)
    #tensor_img = torch.from_numpy(img).float()

    return {'image' : tf(img), 'caption' : tensor_caption}



class Normalize(object):
  """ Custom normalization to images with given mean and 
      standart deviation.
  """

  def __call__(self,sample):
    tensor_img, tensor_caption =  sample['image'], sample['caption']

    # Image normalization on samples, mean's and std's are specifically selected to
    # obey ImageNet means and standart deviations:
    in_transform = transforms.Normalize(mean = [0.485, 0.456, 0.406],
                                        std = [0.229, 0.224, 0.225])
    
    #in_transform = transforms.Compose([transforms.Normalize([0.5],[0.5])])
    
    # Applying normalization:
    tensor_img = in_transform(tensor_img)

    #tensor_img = torch.clip(tensor_img,0,1)


    return {'image' : tensor_img, 'caption' : tensor_caption}

class PreTrainedModels(object):
  """ In this class, state of the art CNN models are placed.
      All models are trained on ImageNet dataset and 1000 classes.
      However, all necessary changes are done in terms of shapes of layer.
      Models : [resnet, alexnet, vgg, squeezenet, densenet, inception]
      Args:         
        num_class (int): Number of classes in the dataset.
         
  """

  def __init__(self,num_classes):

    self.num_classes = num_classes    
    self.model = None
    self.input_size = None

  def set_parameter_requires_grad(self,model,feature_extracting):
    """ İf feature extracking, we set pre-trained parameters's 
        requires_grad = False, since no need to calculate the gra
        dients of the non-updatable parameters.

        Args :
          model (callable)             : PyTorch's torchvision's CNN models
          feature_extracting (Boolean) : True if feature extracking and False if fine-tuning
    """

    if feature_extracting:
      for param in model.parameters():
        param.requires_grad = False


  def ResNet(self,feature_extract = True):
    """ ResNet 18    
    Args:
      feature_extracting (Boolean) : True if feature extracking and False if fine-tuning

    Returns a tuple of :
      ResNet 18 pretained model and it's expected input size
    
    """

    self.model = torchvision.models.resnet152(pretrained=True)
    self.set_parameter_requires_grad(self.model,feature_extract)
    in_ftrs = self.model.fc.in_features
    modules = list(self.model.children())[:-1]      # delete the last fc layer.
    self.model = nn.Sequential(*modules)
    #self.model.fc = nn.Linear(in_ftrs,self.num_classes)
    self.input_size = 224

    return self.model,in_ftrs, self.input_size

  def AlexNet(self,feature_extract = True):
    """ AlexNet
        Args:
          feature_extracting (Boolean) : True if feature extracking and False if fine-tuning

        Returns a tuple of :
          AlexNet pretained model and it's expected input size
       
    """


    self.model = torchvision.models.alexnet(pretrained=True)
    self.set_parameter_requires_grad(self.model,feature_extract)
    in_ftrs = self.model.classifier[6].in_features
    self.model = nn.Sequential(*list(self.model.children())[:-1])    
    self.input_size = 224

    return self.model,in_ftrs, self.input_size

  def VGG(self,feature_extract = True):
    """ VGG11_bn
        Args:
          feature_extracting (Boolean) : True if feature extracking and False if fine-tuning

        Returns a tuple of :
          VGG11_bn pretained model and it's expected input size
    """

    self.model = torchvision.models.vgg19_bn(pretrained=True)
    self.set_parameter_requires_grad(self.model,feature_extract)
    in_ftrs = self.model.classifier[6].in_features
    self.model = nn.Sequential(*list(self.model.children())[:-1])
    self.input_size = 224

    return self.model, self.input_size


  def SqueezeNet(self,feature_extract = True):
    """ Squeezenet
        Args:
          feature_extracting (Boolean) : True if feature extracking and False if fine-tuning

        Returns a tuple of :
          Squeezenet  pretained model and it's expected input size

    """

    self.model = torchvision.models.squeezenet1_0(pretrained=True)
    self.set_parameter_requires_grad(self.model,feature_extract)
    self.model.classifier[1] = nn.Conv2d(512, self.num_classes, kernel_size=(1,1), stride=(1,1))
    self.model.num_classes = self.num_classes
    self.input_size = 224

    return self.model, self.input_size  


  def DenseNet(self,feature_extract = True):
    """ DenseNet
        Args:
          feature_extracting (Boolean) : True if feature extracking and False if fine-tuning

        Returns a tuple of :
          DenseNet  pretained model and it's expected input size

    """
    self.model = models.densenet201(pretrained=True)
    self.set_parameter_requires_grad(self.model, feature_extract)
    num_ftrs = self.model.classifier.in_features
    self.model = nn.Sequential(*list(self.model.children())[:-1])    
    self.input_size = 224

    return self.model,num_ftrs,self.input_size

  
  def Inception_v3(self,feature_extract = True):
    """ Inception v3.
        Be careful, expects (299,299) sized images and has auxiliary output  
        Args:
          feature_extracting (Boolean) : True if feature extracking and False if fine-tuning

        Returns a tuple of :
          Inception_v3  pretained model and it's expected input size      
    """

    self.model = torchvision.models.inception_v3(pretrained=True)
    self.set_parameter_requires_grad(self.model,feature_extract)
    # Handle the auxilary net
    num_ftrs = self.model.AuxLogits.fc.in_features
    self.model.AuxLogits.fc = nn.Linear(num_ftrs, self.num_classes)
    # Handle the primary net
    num_ftrs = self.model.fc.in_features
    self.model.fc = nn.Linear(num_ftrs,self.num_classes)
    self.input_size = 299

    return self.model, self.input_size

class EncoderCNN(nn.Module):
    def __init__(self,in_ftrs,model = None,embed_size = 300):
        super(EncoderCNN, self).__init__()

        if model is not None:
          self.model = model

        else:
          resnet = torchvision.models.resnet18(pretrained=True)
          modules = list(resnet.children())[:-1]      # delete the last fc layer.
          self.model = nn.Sequential(*modules)
          for param in self.model.parameters():
            param.requires_grad = False
          
        self.linear = nn.Linear(in_ftrs, embed_size)
        self.batchNorm = nn.BatchNorm1d(embed_size, momentum=0.01)
 
        # add another fully connected layer
        #self.embed = nn.Linear(in_features=524, out_features=embed_size)
        
        # dropout layer
        #self.dropout = nn.Dropout(0.5)

        
        # activation layers
        #self.prelu = nn.PReLU()

    def forward(self, images):
        
        features = self.model(images)
        features = features.reshape(features.size(0), -1)
        features = self.batchNorm(self.linear(features))
       
        return features

class DecoderRNN(nn.Module):
    def __init__(self, embed_size, hidden_size, vocab_size, torch_embedding = False,num_layers=1):
        super(DecoderRNN, self).__init__()
        
        # define the properties
        self.embed_size = embed_size
        self.hidden_size = hidden_size
        self.vocab_size = vocab_size

        self.embed = nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=self.embed_size)

        if torch_embedding:
          self.embedding_glove6B = torch.load('embedding_glove6B')          
          self.embed.weight.data.copy_(self.embedding_glove6B.vectors)
          self.embed.weight.requires_grad = False

        else:
         
          pretrainedEmbeds = np.loadtxt('embeds300.txt', delimiter=',')
          self.embed.weight.data.copy_(torch.from_numpy(pretrainedEmbeds))
          self.embed.weight.requires_grad = False

        
        # lstm cell
        self.lstm_cell = nn.LSTMCell(input_size = embed_size, hidden_size = hidden_size)
        
        #if num_layers==2:
        self.lstm_cell_layer_2 = nn.LSTMCell(input_size = hidden_size, hidden_size = hidden_size)
    
        # output fully connected layer
        self.fully_connected = nn.Linear(in_features = hidden_size, out_features= vocab_size)
    
        # embedding layer
        #self.embed = nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=self.embed_size)
    
        # activations
        #self.softmax = nn.Softmax(dim=-1)
    
    def forward(self, features, captions):
        
        # batch size
        batch_size = features.size(0)
        
        # init the hidden and cell states to zeros
        hidden_state = torch.zeros((batch_size, self.hidden_size)).to(device)
        cell_state = torch.zeros((batch_size, self.hidden_size)).to(device)
        #hidden_state_layer_2 = torch.zeros((batch_size, self.hidden_size)).to(device)
        #cell_state_layer_2 = torch.zeros((batch_size, self.hidden_size)).to(device)
       
        # define the output tensor placeholder
        outputs = torch.empty((batch_size, captions.size(1), self.vocab_size)).to(device)

        # embed the captions
        captions_embed = self.embed(captions)
        
        # pass the caption word by word
        for t in range(captions.size(1)):

            # for the first time step the input is the feature vector
            if t == 0:
                hidden_state, cell_state = self.lstm_cell(features, (hidden_state, cell_state))
                #hidden_state__layer_2, cell_state_layer_2 = self.lstm_cell_layer_2(hidden_state, (hidden_state_layer_2, cell_state_layer_2))

                
            # for the 2nd+ time step, using teacher forcer
            else:
                hidden_state, cell_state = self.lstm_cell(captions_embed[:, t, :], (hidden_state, cell_state))
                #hidden_state__layer_2, cell_state_layer_2 = self.lstm_cell_layer_2(hidden_state, (hidden_state_layer_2, cell_state_layer_2))
            
            
            # output of the attention mechanism
            out = self.fully_connected(hidden_state)
            
            # build the output tensor
            outputs[:, t, :] = out

               
        return F.log_softmax(outputs, dim = -1)

class DecoderGRU(nn.Module):
    def __init__(self, embed_size, hidden_size, vocab_size, torch_embedding = False,num_layers=1):
        super(DecoderGRU, self).__init__()
        
        # define the properties
        self.embed_size = embed_size
        self.hidden_size = hidden_size
        self.vocab_size = vocab_size

        self.embed = nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=self.embed_size)

        if torch_embedding:
          self.embedding_glove6B = torch.load('embedding_glove6B')          
          self.embed.weight.data.copy_(self.embedding_glove6B.vectors)
          self.embed.weight.requires_grad = False

        else:
         
          pretrainedEmbeds = np.loadtxt('embeds300.txt', delimiter=',')
          self.embed.weight.data.copy_(torch.from_numpy(pretrainedEmbeds))
          self.embed.weight.requires_grad = False

        
        # lstm cell
        #self.lstm_cell = nn.LSTMCell(input_size = embed_size, hidden_size = hidden_size)
        self.GRU_cell = nn.GRUCell(input_size = embed_size, hidden_size = hidden_size)

        #if num_layers==2:
        #self.lstm_cell_layer_2 = nn.LSTMCell(input_size = hidden_size, hidden_size = hidden_size)
    
        # output fully connected layer
        self.fully_connected = nn.Linear(in_features = hidden_size, out_features= vocab_size)

        def forward(self, features, captions):
        
        # batch size
        batch_size = features.size(0)
        
        # init the hidden and cell states to zeros
        hidden_state = torch.zeros((batch_size, self.hidden_size)).to(device)
        cell_state = torch.zeros((batch_size, self.hidden_size)).to(device)
        #hidden_state_layer_2 = torch.zeros((batch_size, self.hidden_size)).to(device)
        #cell_state_layer_2 = torch.zeros((batch_size, self.hidden_size)).to(device)
       
        # define the output tensor placeholder
        outputs = torch.empty((batch_size, captions.size(1), self.vocab_size)).to(device)

        # embed the captions
        captions_embed = self.embed(captions)
        
        # pass the caption word by word
        for t in range(captions.size(1)):

            # for the first time step the input is the feature vector
            if t == 0:
                hidden_state = self.GRU_cell(features, hidden_state)
                

                
            # for the 2nd+ time step, using teacher forcer
            else:
                hidden_state = self.GRU_cell(captions_embed[:, t, :], hidden_state)
                
            
            
            # output of the attention mechanism
            out = self.fully_connected(hidden_state)
            
            # build the output tensor
            outputs[:, t, :] = out

               
        return F.log_softmax(outputs, dim = -1)

class Attention(nn.Module):
    def __init__(self, embed_size, hidden_size, vocab_size, torch_embedding = False,num_layers=1):
        super(Attention, self).__init__()
        
        # define the properties
        self.embed_size = embed_size
        self.hidden_size = hidden_size
        self.vocab_size = vocab_size

        self.embed = nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=self.embed_size)

        if torch_embedding:
          self.embedding_glove6B = torch.load('embedding_glove6B')          
          self.embed.weight.data.copy_(self.embedding_glove6B.vectors)
          self.embed.weight.requires_grad = False

        else:
         
          pretrainedEmbeds = np.loadtxt('embeds300.txt', delimiter=',')
          self.embed.weight.data.copy_(torch.from_numpy(pretrainedEmbeds))
          self.embed.weight.requires_grad = False

        self.lstm = nn.LSTM(input_size = embed_size,hidden_size = hidden_size,
                            num_layers = num_layers, batch_first = True)

        # output fully connected layer
        self.fully_connected = nn.Linear(in_features = hidden_size, out_features= vocab_size)

        self.relu = nn.ReLU()
        
        self.softmax = nn.Softmax(dim=1) 
   
    def forward(self, features, captions):

     
        embed = self.embed(captions)
        embed = torch.cat((features, embed), dim = 1)
        lstm_outputs, hidden = self.lstm(embed)
        out = self.fully_connected(lstm_outputs)
        alpha = self.softmax(att)
        attention_weighted_encoding = (features * alpha.unsqueeze(2)).sum(dim=1)  # (batch_size, encoder_dim)
     
        return attention_weighted_encoding,F.log_softmax(output, dim = -1)

LATENT_SPACE = 300
feature_extract = True
ResNet,in_ftrs,input_size = PreTrainedModels(LATENT_SPACE).ResNet()
encoder_model = EncoderCNN(in_ftrs,ResNet)

# We will be working with GPU:
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

print(device)

preprocess_train = transforms.Compose([RandomHorizontalFlip(),
                                 RandomVerticalFlip(),
                                 Resize(256),
                                 RandomResizedCrop(224),                                 
                                 ToTensor(),
                                 Normalize()                                                           
                               ])

tsfm_train = ImageCaptionData(image_urls = train_url,
                            img_inds = train_inds,
                            captions = train_caps,
                            sentence_lens = sentence_lens_train,
                            transform = preprocess_train
                            )



preprocess_val = transforms.Compose([Resize(224),
                                    ToTensor(),
                                    Normalize()                                                           
                               ])

tsfm_val = ImageCaptionData(image_urls = train_url,
                          img_inds = val_inds,
                          captions = val_caps,
                          sentence_lens = sentence_lens_val,
                          transform = preprocess_val)

num_GPU = torch.cuda.device_count()

print(num_GPU)

BATCH_SIZE = 64

data_transformed = {'train' : tsfm_train, 'val' : tsfm_val}

dataloader = {x : DataLoader(data_transformed[x], batch_size = BATCH_SIZE,
                        shuffle = True, num_workers = 4 * num_GPU , pin_memory = True)
              for x in ['train','val']}

dataset_sizes = {x: len(data_transformed[x]) for x in ['train', 'val']}

# Running on multiple GPU's and distributed processing:
if torch.cuda.device_count() > 1:
  print("Let's use", torch.cuda.device_count(), "GPUs!")
  encoder_model = nn.DataParallel(encoder_model)

encoder_model = encoder_model.to(device)

params_to_update = encoder_model.parameters()

# Just check the uptadable parameters:
if feature_extract:
  params_to_update = []
  for name,param in encoder_model.named_parameters():
    if param.requires_grad == True:
      params_to_update.append(param)
      print("\t",name)
else:
  for name,param in encoder_model.named_parameters():
    if param.requires_grad == True:
      print("\t",name)

# Observe that all parameters are being optimized
encoder_optimizer = optim.Adam(params_to_update,lr = 4e-3)

# Setup the loss function: (may not me used directly)
criterion = nn.CrossEntropyLoss().to(device)

def do_not_run_just_prepo():

  with open('/content/gdrive/My Drive/Data/train/.vector_cache/glove.840B.300d.txt', 'r', encoding="utf8") as f:
    print('1')

    glove = [[str(s) for s in line.rstrip().split(' ')] for line in f]
  print('2')
  words = [g[0] for g in glove]  

  embeds = [np.random.normal(loc=0, scale=0.02, size=300) for _ in np.arange(4)]

  for i in np.arange(1000):
      word = wordDict[i+4]
      if (word == 'xCatch'):
          word = 'catch'
      if (word == 'xWhile'):
          word = 'while'           
      if (word == 'xCase'):
          word = 'case'  
      if (word == 'xEnd'):
          word = 'end'
      if (word == 'xFor'):
          word = 'for'
      index = words.index(word)
      embed = np.array([float(gem) for gem in glove[index][1:]])
      embeds.append(embed)
      
  embeds = np.array(embeds)
  np.savetxt('embeds.840B.300.txt', embeds, delimiter=',')

wordC = pd.read_hdf("/content/gdrive/My Drive/Data/eee443_project_dataset_train.h5", 'word_code')
wordC = wordC.to_dict('split')
wordDict = dict(zip(wordC['data'][0], wordC['columns']))

VOCAB_SIZE = len(wordC['data'][0])
VOCAB_SIZE = 1004 
EMBED_DIM = 300
HIDDEN_DIM = 256

decoder_model = DecoderGRU(EMBED_DIM,HIDDEN_DIM,VOCAB_SIZE)

decoder_optimizer = optim.Adam(decoder_model.parameters(),lr = 1e-3)

# Running on multiple GPU's and distributed processing:
if torch.cuda.device_count() > 1:
  print("Let's use", torch.cuda.device_count(), "GPUs!")
  decoder_model = nn.DataParallel(decoder_model)  

decoder_model = decoder_model.to(device)

# Vanilla schedulers:
decoder_scheduler_LambdaLR = torch.optim.lr_scheduler.LambdaLR(decoder_optimizer, lr_lambda = lambda epoch: 0.9 ** epoch)
encoder_scheduler_LambdaLR = torch.optim.lr_scheduler.LambdaLR(encoder_optimizer, lr_lambda= lambda epoch: 0.9 ** epoch)


# Monitoring val loss:
encoder_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(encoder_optimizer, 'max')
decoder_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(decoder_optimizer, 'max')

decoder = decoder_model
encoder = encoder_model
dataloaders = dataloader
optimizers = [decoder_optimizer,encoder_optimizer]

# The next two functions are part of some other deep learning frameworks, but PyTorch
# has not yet implemented them. We can find some commonly-used open source worked arounds
# after searching around a bit: https://gist.github.com/jihunchoi/f1434a77df9db1bb337417854b398df1.
def _sequence_mask(sequence_length, max_len=None):
    if max_len is None:
        max_len = sequence_length.data.max()
    batch_size = sequence_length.size(0)
    seq_range = torch.arange(0, max_len).long()
    seq_range_expand = seq_range.unsqueeze(0).expand(batch_size, max_len)
    seq_range_expand = Variable(seq_range_expand)
    if sequence_length.is_cuda:
        seq_range_expand = seq_range_expand.cuda()
    seq_length_expand = (sequence_length.unsqueeze(1)
                         .expand_as(seq_range_expand))
    return seq_range_expand < seq_length_expand


def compute_loss(logits, target, length):
    """
    Args:
        logits: A Variable containing a FloatTensor of size
            (batch, max_len, num_classes) which contains the
            unnormalized probability for each class.
        target: A Variable containing a LongTensor of size
            (batch, max_len) which contains the index of the true
            class for each corresponding step.
        length: A Variable containing a LongTensor of size (batch,)
            which contains the length of each data in a batch.

    Returns:
        loss: An average loss value masked by the length.
    """
    # logits_flat: (batch * max_len, num_classes)
    logits_flat = logits.view(-1, logits.size(-1))
    # log_probs_flat: (batch * max_len, num_classes)
    log_probs_flat = logits_flat
    # target_flat: (batch * max_len, 1)
    target_flat = target.view(-1, 1)
    # losses_flat: (batch * max_len, 1)
    losses_flat = -torch.gather(log_probs_flat, dim=1, index=target_flat)
    # losses: (batch, max_len)
    losses = losses_flat.view(*target.size())
    # mask: (batch, max_len)
    mask = _sequence_mask(sequence_length=length, max_len=target.size(1))
    losses = losses * mask.float()
    loss = losses.sum() / length.float().sum()
    return loss

state_lr = 1e-3
decay_lr = 0.99

since = time.time()
PATH = 'models'

PRINT_EVERY = 50

# Number of epochs:
num_epochs = 3

# To keep track history:
val_loss_history = []
train_loss_history = []
val_acc_history = []
train_acc_history = []

best_decoder_wts = copy.deepcopy(decoder.state_dict())
best_encoder_wts = copy.deepcopy(encoder.state_dict())
best_acc = 0.0

if os.path.exists(os.path.join(data_dir,PATH,'1.43_GRU') + '.pth'):
  print('Woring weapons ...')
  checkpoint = torch.load(os.path.join(data_dir,PATH,'1.43_GRU') + '.pth')
  decoder.load_state_dict(checkpoint['Decoder_state_dict'])
  encoder.load_state_dict(checkpoint['Encoder_state_dict'])
  decoder_optimizer.load_state_dict(checkpoint['Decoder_optim_state_dict'])
  encoder_optimizer.load_state_dict(checkpoint['Encoder_optim_state_dict'])
  decoder.to(device)
  encoder.to(device)

for epoch in range(num_epochs):
    print('Epoch {}/{}'.format(epoch + 1, num_epochs))
    print('_' * 10)
    
    if epoch != 0:

      if not os.path.exists(os.path.join(data_dir,PATH,str(epoch)) + ".pth"):
        
        checkpoint = {'Decoder_state_dict' : decoder.state_dict(),
                      'Encoder_state_dict' : encoder.state_dict(),
                      'Decoder_optim_state_dict' : decoder_optimizer.state_dict(),
                      'Encoder_optim_state_dict' : encoder_optimizer.state_dict(),
                      } 

        torch.save(checkpoint, os.path.join(data_dir,PATH,str(epoch)) + ".pth")

        print(f'Model : {epoch} is succesfully saved')
        

    # Each epoch has a training and validation phase
    for phase in ['train','val']:
        if phase == 'train':
            decoder.train()  # Set model to training mode
            encoder.train()
        else:
            decoder.eval()  # Set model to training mode
            encoder.eval()

        running_loss = 0.0
        running_corrects = 0

        # Iterate over data.
        for i,(sample,caption_len) in enumerate(dataloaders[phase]):
            
            images,captions = sample['image'],sample['caption']


            captions_target = captions[:, 1:].long().to(device)
            captions_train = captions[:, :captions.shape[1]-1].long().to(device)
            
            # Move batch of images and captions to GPU if CUDA is available.
            images = images.to(device)

            if phase == 'train':

              # Safer approach of zero gradients in our case
              decoder.zero_grad()
              encoder.zero_grad()
              
              #for optim in optimizers:
                # zero the parameter gradients
                #optim.zero_grad()

              # Pass the inputs through the CNN-RNN model.
              features = encoder(images)
              outputs = decoder(features, captions_train)
              
               
              loss = compute_loss(outputs.contiguous(),
                                  captions_target.contiguous(),
                                  Variable(caption_len.long()).to(device))
              
              #loss = criterion(outputs.view(-1, VOCAB_SIZE), captions_target.contiguous().view(-1))
              
              if i % PRINT_EVERY == 0:
                print('train', loss.item())
                print('train acc : ' , (outputs.argmax(2) == captions_target.data).sum().item()/ (captions_target.size(0) * captions_target.size(1)) * 100)


              # backward + optimize only if in training phase                  
              loss.backward()

              torch.nn.utils.clip_grad_norm(decoder.parameters(),10.0)
              torch.nn.utils.clip_grad_norm(encoder.parameters(),10.0)

              for optim in optimizers:
                # zero the parameter gradients                    
                optim.step()

            if phase == 'val':
                           
              with torch.no_grad():
                # Pass the inputs through the CNN-RNN model.
                features = encoder(images)
                outputs = decoder(features, captions_train)

                # Calculate the batch loss
                loss = compute_loss(outputs.contiguous(),
                                  captions_target.contiguous(),
                                  Variable(caption_len.long()).to(device))
                
                if i % PRINT_EVERY == 0:
                  print('Val',loss.item())
                  print('Val correct' , (outputs.argmax(2) == captions_target.data).sum().item()/ (captions_target.size(0) * captions_target.size(1)) * 100)

              
                                
            # statistics
            running_loss += loss.item() * images.size(0)
            running_corrects += (outputs.argmax(2) == captions_target.data).sum().item()/ (captions_target.size(0) * captions_target.size(1))
            #print('running_corrects',running_corrects)

            if i % 1000 == 0:
               
              for optim in optimizers:
                for param_group in optim.param_groups:                  
                  param_group['lr'] = state_lr
              
              state_lr *= decay_lr
              print('state_lr ',state_lr)
              

        epoch_loss = running_loss / len(dataloaders[phase].dataset)
        epoch_acc = (running_corrects / len(dataloaders[phase].dataset)) * 100
        
        decoder_scheduler_LambdaLR.step()
        encoder_scheduler_LambdaLR.step()  
        decoder_scheduler.step(epoch_acc)
        encoder_scheduler.step(epoch_acc) 
        

        if phase == 'val':        
          val_loss_history.append(epoch_loss)
          val_acc_history.append(epoch_acc)

        if phase == 'train':
          train_loss_history.append(epoch_loss)
          train_acc_history.append(epoch_acc)
          

        print('{} Loss: {:.4f} '.format(phase, epoch_loss))
        print('{} Acc: {:.4f} '.format(phase, epoch_acc))

        # deep copy the model
        if phase == 'val' and epoch_acc > best_acc:
            best_acc = epoch_acc                
            best_decoder_wts = copy.deepcopy(decoder.state_dict())
            best_encoder_wts = copy.deepcopy(encoder.state_dict())
            
        
    print()

time_elapsed = time.time() - since
print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))


# load best model weights
#decoder.load_state_dict(best_decoder_wts)
#encoder.load_state_dict(best_encoder_wts)


torch.save({'Decoder_state_dict' : decoder.state_dict(),
            'Encoder_state_dict' : encoder.state_dict(),
            'Decoder_optim_state_dict' : decoder_optimizer.state_dict(),
            'Encoder_optim_state_dict' : encoder_optimizer.state_dict(),
              },           
            os.path.join(data_dir,PATH,'final_model_GRU') + ".pth")

LSTM_path = 'best_model_sensible'
GRU_path = '2GRU'
Best_GRU_path = 'SON_GRU'

batch,_ = next(iter(dataloader['val']))
if os.path.exists(os.path.join(data_dir,'models',GRU_path) + '.pth'):
  print('Working...')
  checkpoint = torch.load(os.path.join(data_dir,'models',GRU_path) + '.pth', map_location=torch.device('cpu'))
  decoder.load_state_dict(checkpoint['Decoder_state_dict'])
  encoder.load_state_dict(checkpoint['Encoder_state_dict'])
  decoder.to(device)
  encoder.to(device)

with torch.no_grad():
  decoder.eval()
  encoder.eval()
  images,captions = batch['image'],batch['caption']


  captions_target = captions[:, 1:].long().to(device)
  captions_train = captions[:, :-1].long().to(device)

  # Move batch of images and captions to GPU if CUDA is available.
  images = images.to(device)

  features = encoder(images)
  outputs = decoder(features, captions_train) 

  greedy_outputs = outputs.argmax(2)

words = pd.read_hdf("/content/gdrive/My Drive/Data/eee443_project_dataset_train.h5", 'word_code')
words = words.to_dict('split')
wordDict = dict(zip(words['data'][0], words['columns']))

def generate_caption_argmax_search(images,greedy_outputs,captions_target, index = None, METEOR = False, n_gram_bleus = False,cumulative_bleus = False):
  if index is None:
    index = np.random.randint(BATCH_SIZE)
  
  
  caption = [wordDict[i] for i in captions_target[index].cpu().detach().numpy()]
  
  captionOut = [wordDict[i] for i in greedy_outputs.cpu().detach().numpy()[index]]

  try:    
    hypothesis = captionOut[:captionOut.index('x_END_')]
  except ValueError:
    hypothesis = captionOut[:captionOut.index('x_NULL_')]

  reference = caption[:caption.index('x_END_')]

  BLEUscore = sentence_bleu([reference], hypothesis)
  
  if METEOR:
    METEOR = meteor_score([reference], hypothesis)
    print(METEOR)

  plt.figure(figsize=(8,6))
  
  plt.imshow(images[index].permute(1,2,0).cpu())
  plt.title('Reference caption:  ' + ' '.join(reference))

  plt.xlabel('Generated Caption: ' + ' '.join(hypothesis) + '\n BLEU Score:' + str(BLEUscore))

  if n_gram_bleus: 
    print('BLEU1 Score: %f' % sentence_bleu(reference, hypothesis, weights=(1, 0, 0, 0)))
    print('BLEU2 Score: %f' % sentence_bleu(reference, hypothesis, weights=(0, 1, 0, 0)))
    print('BLEU3 Score: %f' % sentence_bleu(reference, hypothesis, weights=(0, 0, 1, 0)))
    print('BLEU4 Score: %f' % sentence_bleu(reference, hypothesis, weights=(0, 0, 0, 1)))
    

  if cumulative_bleus:
    print('Cumulative BLEU : %f' % sentence_bleu(reference, hypothesis, weights=(0.25, 0.25, 0.25, 0.25)))

generate_caption_argmax_search(images,greedy_outputs,captions_target, index = 32)

# beam search
def beam_search_decoder(data, k):
	sequences = [[list(), 0.0]]
	# walk over each step in sequence
	for row in data:
		all_candidates = list()
		# expand each current candidate
		for i in range(len(sequences)):
			seq, score = sequences[i]
			for j in range(len(row)):
				candidate = [seq + [j], score - np.log(row[j] + 50)]
				all_candidates.append(candidate)
		# order all candidates by score
		ordered = sorted(all_candidates, key=lambda tup:tup[1])
		# select k best
		sequences = ordered[:k]
	return sequences

def beam_search_decoder_alternative(predictions, top_k = 3):
    
    #start with an empty sequence with zero score
    output_sequences = [([], 0)]
    
    #looping through all the predictions
    for token_probs in predictions:
        new_sequences = []
        
        #append new tokens to old sequences and re-score
        for old_seq, old_score in output_sequences:
            for char_index in range(len(token_probs)):
                new_seq = old_seq + [char_index]
                #considering log-likelihood for scoring
                new_score = old_score + np.log(token_probs[char_index] + 50)
                new_sequences.append((new_seq, new_score))
                
        #sort all new sequences in the de-creasing order of their score
        output_sequences = sorted(new_sequences, key = lambda val: val[1], reverse = True)
        
        #select top-k based on score 
        # *Note- best sequence is with the highest score
        output_sequences = output_sequences[:top_k]
        
    return output_sequences

def generate_caption_beam_search(images,raw_outputs,captions_target, 
                                 index = None, beam_size = 3,
                                 METEOR = False, n_gram_bleus = False, cumulative_bleus = False,
                                 print_bleu = False):
  
  if index is None:
    index = np.random.randint(BATCH_SIZE)

  plt.figure(figsize=(8,6))
  
  caption = [wordDict[i] for i in captions_target[index].cpu().detach().numpy()]

 

  
  beam_decoded = beam_search_decoder(raw_outputs[index].cpu().detach().numpy(),k = beam_size)

  captionOut = [seq[0] for seq in beam_decoded]
  captionOutList = []
  for cap_inds in captionOut:
    captionOutList.append([wordDict[i] for i in cap_inds])

  hypothesisList = []

  for captions in captionOutList:
    try:    
      hypothesis = captions[:captions.index('x_END_')]
      hypothesisList.append(hypothesis)
    except ValueError:
      hypothesis = captions[:captions.index('x_NULL_')]
      hypothesisList.append(hypothesis)

  reference = caption[:caption.index('x_END_')]

  
  
  if print_bleu:
    BLEUscore = sentence_bleu([reference], hypothesis)
    print('BLEUscore :' + str(BLEUscore))
   
  
  if METEOR:
    METEOR = meteor_score([reference], hypothesis)
    print(METEOR)
  
  plt.figure()
  
  plt.imshow(images[index].permute(1,2,0).cpu())
  plt.title('Reference caption:  ' + ' '.join(reference))
  
  if beam_size == 3:
    plt.xlabel('Generated Captions for beam size = '  + str(beam_size) + '\n' +
            ' '.join(hypothesisList[0]) + '\n' + 
            ' '.join(hypothesisList[1]) + '\n' +  
            ' '.join(hypothesisList[2]) + '\n' 
            )
  
  if beam_size == 5:
    plt.xlabel('Generated Captions for beam size= '  + str(beam_size) + '\n' +
            ' '.join(hypothesisList[0]) + '\n' + 
            ' '.join(hypothesisList[1]) + '\n' +  
            ' '.join(hypothesisList[2]) + '\n' + 
            ' '.join(hypothesisList[3]) + '\n' + 
            ' '.join(hypothesisList[4]) + '\n' 
            )
    


  if beam_size == 7:
    plt.xlabel('Generated Captions for beam size = '  + str(beam_size) + '\n' +
            ' '.join(hypothesisList[0]) + '\n' + 
            ' '.join(hypothesisList[1]) + '\n' +  
            ' '.join(hypothesisList[2]) + '\n' + 
            ' '.join(hypothesisList[3]) + '\n' + 
            ' '.join(hypothesisList[4]) + '\n' +
            ' '.join(hypothesisList[5]) + '\n' + 
            ' '.join(hypothesisList[6]) + '\n' 
            )
  if beam_size == 9:
    plt.xlabel('Generated Captions for beam size= '  + str(beam_size) + '\n' +
            ' '.join(hypothesisList[0]) + '\n' + 
            ' '.join(hypothesisList[1]) + '\n' +  
            ' '.join(hypothesisList[2]) + '\n' + 
            ' '.join(hypothesisList[3]) + '\n' + 
            ' '.join(hypothesisList[4]) + '\n' +
            ' '.join(hypothesisList[5]) + '\n' + 
            ' '.join(hypothesisList[6]) + '\n' +
            ' '.join(hypothesisList[7]) + '\n' + 
            ' '.join(hypothesisList[8]) + '\n'  
            )
    
  if n_gram_bleus: 
    print('BLEU1 Score: %f' % sentence_bleu(reference, hypothesis, weights=(1, 0, 0, 0)))
    print('BLEU2 Score: %f' % sentence_bleu(reference, hypothesis, weights=(0, 1, 0, 0)))
    print('BLEU3 Score: %f' % sentence_bleu(reference, hypothesis, weights=(0, 0, 1, 0)))
    print('BLEU4 Score: %f' % sentence_bleu(reference, hypothesis, weights=(0, 0, 0, 1)))
    

  if cumulative_bleus:
    print('Cumulative BLEU : %f' % sentence_bleu(reference, hypothesis, weights=(0.25, 0.25, 0.25, 0.25)))

generate_caption_beam_search(images,outputs,captions_target,index = 5,beam_size=3)
generate_caption_argmax_search(images,greedy_outputs,captions_target, index = 5)

# Getting the data:
with h5py.File(colap_path_test,'r') as f:
# Names variable contains the names of training and testing file 
    names = list(f.keys())
    test_caps = f[names[0]][()]
    test_imid = np.array(f[names[1]][()])
    train_imid -=1
    test_url = np.array(f[names[3]][()])

test_cap_Str = {}
for i in range(len(test_caps)):
  test_cap_Str[i] = [element for element in test_caps[i] if element != 0]
sentence_lens_test = [len(ele) for ele in test_cap_Str.values()]

preprocess_test = transforms.Compose([Resize(224),
                                    ToTensor(),
                                    #Normalize()                                                           
                               ])

tsfm_test = ImageCaptionData(image_urls = test_url,
                          img_inds = test_imid,
                          captions = test_caps,
                          sentence_lens = sentence_lens_test,
                          transform = preprocess_test)

dataloader_test = DataLoader(tsfm_test, batch_size = BATCH_SIZE,
                        shuffle = True, num_workers = 4 * num_GPU , pin_memory = True)

batch,_ = next(iter(dataloader_test)
if os.path.exists(os.path.join(data_dir,'models',GRU_path) + '.pth'):
  print('Working...')
  checkpoint = torch.load(os.path.join(data_dir,'models',GRU_path) + '.pth', map_location=torch.device('cpu'))
  decoder.load_state_dict(checkpoint['Decoder_state_dict'])
  encoder.load_state_dict(checkpoint['Encoder_state_dict'])
  decoder.to(device)
  encoder.to(device)

with torch.no_grad():
  decoder.eval()
  encoder.eval()
  images,captions = batch['image'],batch['caption']


  captions_target = captions[:, 1:].long().to(device)
  captions_train = captions[:, :-1].long().to(device)

  # Move batch of images and captions to GPU if CUDA is available.
  images = images.to(device)

  features = encoder(images)
  outputs = decoder(features, captions_train) 

  greedy_outputs = outputs.argmax(2)

generate_caption_beam_search(images,outputs,captions_target,index = 5,beam_size=3)
generate_caption_argmax_search(images,greedy_outputs,captions_target, index = 5)
